# ğŸš€ Proyecto Integrador - Pipeline ELT y Data Warehouse Escalable

## ğŸ“Œ Contexto del Proyecto

Como Ingeniero de Datos en una empresa en expansiÃ³n, se diseÃ±Ã³ e implementÃ³ un pipeline de datos tipo **ELT (Extract, Load, Transform)** desplegado en la nube.  
El objetivo del proyecto es integrar mÃºltiples fuentes de datos, transformarlas y almacenarlas en un **Data Warehouse escalable en capas**, orquestado con **Apache Airflow** y automatizado mediante **GitHub Actions (CI/CD)**.

---

## ğŸ¯ Objetivos del Proyecto

- DiseÃ±ar e implementar un pipeline ELT en batch.
- Integrar datos desde mÃºltiples fuentes (CSV y API).
- Construir un Data Warehouse en capas (Bronze, Silver, Gold).
- Aplicar transformaciones mediante SQL.
- Orquestar el pipeline con Apache Airflow.
- Implementar CI/CD con GitHub Actions.
- Generar anÃ¡lisis y dashboard en Jupyter Notebook.

---

## ğŸ—ï¸ Arquitectura del Pipeline

**Fuentes de Datos â†’ S3 (Bronze) â†’ MySQL RDS (Silver & Gold) â†’ Analytics Notebook**

### Capas del Data Warehouse

| Capa       | DescripciÃ³n                                 |
| ---------- | ------------------------------------------- |
| **Bronze** | Datos crudos almacenados en S3 (CSV y JSON) |
| **Silver** | Datos limpiados y transformados en MySQL    |
| **Gold**   | Modelo dimensional optimizado para anÃ¡lisis |

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- Python
- AWS S3
- AWS RDS MySQL
- Apache Airflow
- Docker & Docker Compose
- SQL (MySQL)
- GitHub Actions (CI/CD)
- Jupyter Notebook

---

## âš™ï¸ Pipeline ELT

### 1ï¸âƒ£ Extract & Load (Bronze)

- ExtracciÃ³n de datos desde:
  - Dataset Airbnb NYC (CSV)
  - API de tasas de cambio (JSON)
- Almacenamiento en AWS S3.

### 2ï¸âƒ£ Transform & Load (Silver)

- Limpieza y normalizaciÃ³n de datos.
- ConversiÃ³n de precios a mÃºltiples monedas.
- Carga a MySQL RDS.

### 3ï¸âƒ£ Data Warehouse (Gold)

- Modelo dimensional tipo Star Schema:
  - Tablas Dimensionales
  - Tabla de Hechos
  - Data Mart final para analÃ­tica

---

## ğŸ“Š AnÃ¡lisis de Negocio

Se responden preguntas estratÃ©gicas como:

- Â¿CuÃ¡les son las zonas mÃ¡s rentables?
- Â¿CuÃ¡l es el precio promedio por barrio?
- Â¿QuÃ© tipo de alojamiento es mÃ¡s demandado?
- Â¿QuiÃ©nes son los hosts con mayor nÃºmero de propiedades?

---

## ğŸ¤– OrquestaciÃ³n y CI/CD

### Apache Airflow

- DAGs definidos para:
  - Ingesta
  - TransformaciÃ³n
  - Carga en Data Warehouse

### GitHub Actions

- Pipeline CI/CD para:
  - ValidaciÃ³n de cÃ³digo
  - Linting
  - AutomatizaciÃ³n de despliegues

---

## ğŸ“ˆ Dashboard

Se desarrollÃ³ un notebook en **Jupyter Notebook** que consulta el Data Warehouse y genera visualizaciones para responder preguntas de negocio.

---

## ğŸ‘¤ Autor

**Antonio Plata**  
Bootcamp Data Engineer - Henry

## ğŸ“‚ Estructura del Proyecto

```text
airflow_project/
â”‚
â”œâ”€â”€ dags/                  # DAGs de Apache Airflow
â”‚   â”œâ”€â”€ bronze_upload_dag.py
â”‚   â”œâ”€â”€ silver_load_dag.py
â”‚   â””â”€â”€ gold_transform_dag.py
â”‚
â”œâ”€â”€ scripts/               # Scripts Python de ingestiÃ³n y carga
â”‚   â”œâ”€â”€ api_aws_bucket_env.py
â”‚   â”œâ”€â”€ local_upload_bucket_env.py
â”‚   â”œâ”€â”€ load_airbnb_env.py
â”‚   â””â”€â”€ load_exchange_env.py
â”‚
â”œâ”€â”€ sql/                   # Scripts SQL del Data Warehouse
â”‚   â””â”€â”€ gold_layer.sql
â”‚
â”œâ”€â”€ notebooks/             # Dashboard y anÃ¡lisis
â”‚   â””â”€â”€ analysis.ipynb
â”‚
â”œâ”€â”€ docs/                  # DocumentaciÃ³n por avances
â”‚   â”œâ”€â”€ avance_1_ingestion.md
â”‚   â”œâ”€â”€ avance_2_etl_pipeline.md
â”‚   â”œâ”€â”€ avance_3_data_warehouse.md
â”‚   â””â”€â”€ avance_4_analytics.md
â”‚
â”œâ”€â”€ docker-compose.yml     # Infraestructura Airflow
â”œâ”€â”€ Dockerfile              # ContenerizaciÃ³n de scripts
â”œâ”€â”€ .env.example            # Variables de entorno ejemplo
â”œâ”€â”€ .gitignore

---
```
