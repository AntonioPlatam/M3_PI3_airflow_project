# Avance 1 ‚Äì Dise√±o de la Arquitectura del Pipeline ELT

## üìå Contexto del Proyecto

Este proyecto corresponde al **Proyecto Integrador del m√≥dulo de Data Engineering**, cuyo objetivo es dise√±ar e implementar un **pipeline ELT escalable y un Data Warehouse en la nube**.

El rol asumido es el de **Ingeniero de Datos**, responsable de definir la arquitectura del sistema que permitir√° integrar, transformar y analizar grandes vol√∫menes de datos provenientes de m√∫ltiples fuentes.

El caso de estudio se centra en el an√°lisis del mercado de hospedaje (Airbnb NYC), con el fin de responder preguntas de negocio relacionadas con precios, demanda y comportamiento de hosts.

---

## üéØ Objetivo del Pipeline ELT

Dise√±ar una arquitectura escalable que permita:

- Ingerir datos desde m√∫ltiples fuentes (CSV y APIs).
- Almacenar los datos crudos en una capa Raw.
- Transformar los datos en un Data Warehouse estructurado por capas.
- Automatizar el flujo con Apache Airflow.
- Implementar CI/CD con GitHub Actions.
- Garantizar calidad, trazabilidad y reproducibilidad del pipeline.

---

## üîÑ Enfoque ELT (Extract, Load, Transform)

El pipeline sigue la metodolog√≠a **ELT**, donde los datos se cargan primero en bruto y luego se transforman dentro del ecosistema de datos.

### üîπ Extract

Fuentes de datos utilizadas:

- Archivo CSV: `AB_NYC.csv` (dataset de Airbnb NYC).
- API externa de tasas de cambio (CurrencyFreaks).

La extracci√≥n se realiza mediante scripts en Python, ejecutados dentro de contenedores Docker.

---

### üîπ Load

Los datos extra√≠dos se almacenan en **Amazon S3 (capa Bronze)** sin transformaciones, conservando el formato original.

Prop√≥sito:

- Preservar el dato original.
- Permitir reprocesamiento futuro.
- Garantizar trazabilidad.

---

### üîπ Transform

Las transformaciones se realizan en **Amazon RDS (MySQL)** estructurando los datos en capas:

| Capa   | Tecnolog√≠a | Descripci√≥n                                 |
| ------ | ---------- | ------------------------------------------- |
| Bronze | Amazon S3  | Datos crudos sin transformar                |
| Silver | Amazon RDS | Datos limpios y normalizados                |
| Gold   | Amazon RDS | Datos agregados y optimizados para an√°lisis |

---

## üèóÔ∏è Arquitectura General del Pipeline

La arquitectura del pipeline est√° compuesta por los siguientes componentes:

1. **Fuentes de Datos**
   - CSV local (Airbnb NYC)
   - API externa de tasas de cambio

2. **Ingesta**
   - Scripts en Python
   - Contenedores Docker

3. **Almacenamiento Raw**
   - Amazon S3 (Bronze Layer)

4. **Transformaci√≥n y Data Warehouse**
   - Amazon RDS MySQL
   - Capas Silver y Gold

5. **Orquestaci√≥n**
   - Apache Airflow para automatizar el pipeline

6. **CI/CD**
   - GitHub Actions para automatizar pruebas y despliegue

7. **Consumo**
   - Jupyter Notebook para an√°lisis y dashboards

---

## üß± Capas del Data Warehouse

### ü•â Bronze Layer (Raw)

- Servicio: Amazon S3
- Contenido: Datos crudos CSV y JSON
- Pr
