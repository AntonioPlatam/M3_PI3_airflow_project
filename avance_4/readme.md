# Avance 4 â€“ OrquestaciÃ³n del Pipeline y CI/CD

## ğŸ“Œ Contexto del Avance

En este avance se implementÃ³ la orquestaciÃ³n completa del pipeline ELT utilizando Apache Airflow, junto con la contenerizaciÃ³n mediante Docker y la automatizaciÃ³n de pruebas mediante GitHub Actions.

El objetivo fue automatizar la ejecuciÃ³n del pipeline desde la ingesta de datos hasta la generaciÃ³n de la capa Gold, garantizando reproducibilidad, escalabilidad y mantenibilidad.

---

## ğŸ³ ContenerizaciÃ³n del Pipeline con Docker

Se configurÃ³ un entorno completo de Apache Airflow utilizando Docker Compose y un Dockerfile personalizado.

### ğŸ“‚ Archivos Implementados

| Archivo              | DescripciÃ³n                                        |
| -------------------- | -------------------------------------------------- |
| `docker-compose.yml` | Infraestructura de Airflow con PostgreSQL y Redis  |
| `Dockerfile`         | Imagen personalizada con dependencias del pipeline |

---

## â±ï¸ OrquestaciÃ³n con Apache Airflow

Se definieron DAGs modulares para cada capa del Data Warehouse, siguiendo la arquitectura ELT.

### ğŸ§© DAGs Implementados

| DAG                      | DescripciÃ³n                                         |
| ------------------------ | --------------------------------------------------- |
| `dag_bronze.py`          | Ingesta de datos desde API y CSV hacia S3 (Bronze)  |
| `dag_silver.py`          | Carga de datos desde S3 hacia RDS MySQL (Silver)    |
| `dag_gold.py`            | Transformaciones SQL y creaciÃ³n de Data Mart (Gold) |
| `dag_pipeline_master.py` | Orquestador maestro del pipeline completo           |

---

### ğŸ§  Flujo del Pipeline

1. **Bronze Layer**
   - ExtracciÃ³n de datos desde API (CurrencyFreaks)
   - Carga del dataset Airbnb desde archivo local
   - Persistencia en Amazon S3

2. **Silver Layer**
   - Lectura de archivos desde S3
   - Carga en MySQL RDS

3. **Gold Layer**
   - Transformaciones SQL
   - Modelo dimensional (Star Schema)
   - CreaciÃ³n de Big Table analÃ­tica

---

## ğŸ—„ï¸ Transformaciones Automatizadas

Se automatizÃ³ la ejecuciÃ³n del script SQL de la capa Gold utilizando Python.

### ğŸ“‚ Archivo

| Archivo           | DescripciÃ³n                                     |
| ----------------- | ----------------------------------------------- |
| `run_gold_sql.py` | Ejecuta `gold_layer.sql` en MySQL desde Airflow |

---

## ğŸ”„ CI/CD con GitHub Actions

Se configurÃ³ un pipeline de integraciÃ³n continua para validar el cÃ³digo en cada push o pull request.

### ğŸ“‚ Archivo

| Archivo                    | DescripciÃ³n                          |
| -------------------------- | ------------------------------------ |
| `.github/workflows/ci.yml` | Pipeline CI para pruebas automÃ¡ticas |

### ğŸ§ª Validaciones Implementadas

- InstalaciÃ³n de dependencias Python
- Linting del cÃ³digo
- ValidaciÃ³n de scripts Python y SQL

---

## ğŸ“Š Dashboard AnalÃ­tico en Jupyter Notebook

Se desarrollÃ³ un Notebook para responder preguntas de negocio utilizando la capa Gold del Data Warehouse.

### ğŸ“‚ Archivo

| Archivo       | DescripciÃ³n                                             |
| ------------- | ------------------------------------------------------- |
| `final.ipynb` | Dashboard analÃ­tico con consultas SQL y visualizaciones |

---

## ğŸ“ˆ Preguntas de Negocio Respondidas

- Zonas mÃ¡s rentables de Nueva York
- Precio promedio por barrio
- Tipos de alojamiento mÃ¡s demandados
- Hosts con mayor nÃºmero de propiedades

Las consultas se ejecutan sobre la tabla:

```sql
gold_airbnb_master
```
